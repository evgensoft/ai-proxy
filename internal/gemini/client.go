package gemini

import (
	"bytes"
	"encoding/json"
	"fmt"
	"log"
	"net/http"
	"time"

	"ai-proxy/internal/schema"
)

var (
	queue           = make(chan interface{}, 1)
	lastTimeRequest = time.Now()

	maxTimeoutTime = 10 * time.Second
)

type RequestAutoGenerated struct {
	Contents []Contents `json:"contents,omitempty"`
	// SystemInstruction Contents   `json:"systemInstruction,omitempty"`
}

type Parts struct {
	Text string `json:"text,omitempty"`
}

type Contents struct {
	Role  string  `json:"role,omitempty"`
	Parts []Parts `json:"parts,omitempty"`
}

type SystemInstruction struct {
	Contents []Contents `json:"contents,omitempty"`
}

type ResponceGenerated struct {
	Candidates []struct {
		Content struct {
			Parts []struct {
				Text string `json:"text,omitempty"`
			} `json:"parts,omitempty"`
			Role string `json:"role,omitempty"`
		} `json:"content,omitempty"`
		FinishReason string `json:"finishReason,omitempty"`
		Index        int    `json:"index,omitempty"`
	} `json:"candidates,omitempty"`
	UsageMetadata struct {
		PromptTokenCount     int `json:"promptTokenCount,omitempty"`
		CandidatesTokenCount int `json:"candidatesTokenCount,omitempty"`
		TotalTokenCount      int `json:"totalTokenCount,omitempty"`
	} `json:"usageMetadata,omitempty"`
	Error struct {
		Code    int    `json:"code,omitempty"`
		Message string `json:"message,omitempty"`
		Status  string `json:"status,omitempty"`
	} `json:"error,omitempty"`
}

func CreateRequest(providerURL, model, token string, reqBody schema.RequestOpenAICompatable) (*http.Request, error) {
	var body RequestAutoGenerated

	for _, v := range reqBody.Messages {
		switch v.Role {
		// case "system":
		// 	body.SystemInstruction = Contents{
		// 		Role:  "user",
		// 		Parts: []Parts{{Text: v.Content}},
		// 	}
		case "system":
			body.Contents = append(body.Contents, Contents{
				Role:  "user",
				Parts: []Parts{{Text: v.Content}},
			})
		case "user":
			body.Contents = append(body.Contents, Contents{
				Role:  v.Role,
				Parts: []Parts{{Text: v.Content}},
			})
		case "assistant":
			body.Contents = append(body.Contents, Contents{
				Role:  "model",
				Parts: []Parts{{Text: v.Content}},
			})
		default:
			body.Contents = append(body.Contents, Contents{
				Role:  v.Role,
				Parts: []Parts{{Text: v.Content}},
			})
		}
	}

	jsonBody, err := json.Marshal(body)
	if err != nil {
		return nil, err
	}

	// fmt.Printf("JSON BODY: %s\n", string(jsonBody))

	providerURL = fmt.Sprintf("%s/%s:generateContent?key=%s", providerURL, model, token)

	return http.NewRequest(http.MethodPost, providerURL, bytes.NewReader(jsonBody))
}

func Call(providerURL, model, token string, reqBody schema.RequestOpenAICompatable) ([]byte, error) {
	// Выполняем запросы в 1 поток
	queue <- struct{}{}
	defer func() {
		// Запоминаем время последнего запроса и очищаем очередь
		lastTimeRequest = time.Now()

		<-queue
	}()

	if !time.Now().After(lastTimeRequest.Add(maxTimeoutTime)) {
		log.Printf("Throttled %v seс for %s", time.Until(lastTimeRequest.Add(maxTimeoutTime)).Seconds(), model)
		time.Sleep(time.Until(lastTimeRequest.Add(maxTimeoutTime)))
	}

	req, err := CreateRequest(providerURL, model, token, reqBody)
	if err != nil {
		return nil, err
	}

	resp, err := http.DefaultClient.Do(req)
	if err != nil {
		return nil, err
	}

	defer resp.Body.Close()

	var response ResponceGenerated

	err = json.NewDecoder(resp.Body).Decode(&response)
	if err != nil {
		return nil, err
	}

	return transformResponse(response)
}

func transformResponse(response ResponceGenerated) ([]byte, error) {
	if response.Error.Message != "" {
		return nil, fmt.Errorf("error code %d - %s", response.Error.Code, response.Error.Message)
	}

	if len(response.Candidates) == 0 {
		return nil, fmt.Errorf("no candidates")
	}

	type Message struct {
		Role    string `json:"role"`
		Content string `json:"content"`
	}

	type Choices struct {
		Index        int     `json:"index"`
		Message      Message `json:"message"`
		FinishReason string  `json:"finish_reason"`
		Logprobs     any     `json:"logprobs"`
	}

	type Usage struct {
		PromptTokens     int `json:"prompt_tokens"`
		CompletionTokens int `json:"completion_tokens"`
		TotalTokens      int `json:"total_tokens"`
	}

	type ContentResponse struct {
		ID      string    `json:"id"`
		Model   string    `json:"model,omitempty"`
		Object  string    `json:"object"`
		Created int       `json:"created"`
		Choices []Choices `json:"choices"`
		Usage   Usage     `json:"usage"`
	}

	var respTxt ContentResponse

	for _, v := range response.Candidates {
		var ch Choices

		ch.Index = v.Index
		if v.Content.Role == "model" {
			ch.Message.Role = "assistant"
		} else {
			ch.Message.Role = v.Content.Role
		}

		ch.Message.Content = v.Content.Parts[0].Text
		ch.FinishReason = v.FinishReason

		respTxt.Choices = append(respTxt.Choices, ch)
	}

	respTxt.Usage.PromptTokens = response.UsageMetadata.PromptTokenCount
	respTxt.Usage.CompletionTokens = response.UsageMetadata.CandidatesTokenCount
	respTxt.Usage.TotalTokens = response.UsageMetadata.TotalTokenCount

	return json.Marshal(respTxt)
}
